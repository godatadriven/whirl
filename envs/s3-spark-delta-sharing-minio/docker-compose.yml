version: '3'

services:
  airflow:
    image: docker-whirl-airflow:py-${PYTHON_VERSION}-local
    command: ["singlemachine"]
    ports:
      - '5000:5000'  # HTTP (Airflow Web UI)
    env_file:
      - .whirl.env
    environment:
      - AIRFLOW__API__AUTH_BACKEND
    volumes:
      - ${DAG_FOLDER}:/opt/airflow/dags/$PROJECTNAME
      - ${ENVIRONMENT_FOLDER}/whirl.setup.d:${WHIRL_SETUP_FOLDER}/env.d/
      - ${DAG_FOLDER}/whirl.setup.d:${WHIRL_SETUP_FOLDER}/dag.d/
      - ${MOCK_DATA_FOLDER}:/mock-data
    depends_on:
      - s3server
      - sparkmaster
    links:
      - s3server:${DEMO_BUCKET}.s3server


  s3server:
    image: minio/minio:RELEASE.2023-06-19T19-52-50Z
    command:
      - server
      - "--address=0.0.0.0:9000"
      - "/tmp/minio"
    ports:
      - 9000:9000
    environment:
      - MINIO_ACCESS_KEY=${AWS_ACCESS_KEY_ID}
      - MINIO_SECRET_KEY=${AWS_SECRET_ACCESS_KEY}
    volumes:
      - ./.s3-mount:/tmp/minio

  delta:
    #image: deltaio/delta-sharing-server:${DELTA_SHARING_VERSION}
    image: krisgeus/delta-sharing-server:1.0.0-SNAPSHOT
    command: ["--config", "/opt/docker/conf/delta-sharing.yml"]
    ports:
      - 38080:8080
    environment:
      - HADOOP_ROOT_LOGGER=ERROR,console
      - JAVA_OPTS=-Dlog4j.debug=false -Dlog4j.logLevel=INFO -Dlog4j.configuration=file:///opt/docker/conf/log4j.properties -Dlog4j.configurationFile=file:///opt/docker/conf/log4j.properties
    volumes:
      - ./config/:/opt/docker/conf/

  sparkmaster:
    build:
      context: ${DOCKER_CONTEXT_FOLDER}/aws-spark
      dockerfile: Dockerfile
      args:
        - SPARK_VERSION=${SPARK_VERSION}
    environment:
      - SPARK_NO_DAEMONIZE=true
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - AWS_SERVER
      - AWS_PORT
    ports:
      - 7077:7077
      - 18080:8080
    entrypoint:
      - /usr/spark/sbin/start-master.sh
    links:
      - s3server:${DEMO_BUCKET}.s3server

  sparkworker:
    build:
      context: ${DOCKER_CONTEXT_FOLDER}/aws-spark
      dockerfile: Dockerfile
      args:
        - SPARK_VERSION=${SPARK_VERSION}
    environment:
      - SPARK_NO_DAEMONIZE=true
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - AWS_SERVER
      - AWS_PORT
    ports:
      - 18081:8081
    entrypoint:
      - /usr/spark/sbin/start-slave.sh
      - spark://sparkmaster:7077
      - "-m"
      - "8G"
    depends_on:
      - sparkmaster
    links:
      - s3server:${DEMO_BUCKET}.s3server

  sparkshell:
    build:
      context: ${DOCKER_CONTEXT_FOLDER}/aws-spark
      dockerfile: Dockerfile
      args:
        - SPARK_VERSION=${SPARK_VERSION}
    environment:
      - SPARK_NO_DAEMONIZE=true
    entrypoint:
      - /usr/bin/tail
      - "-f"
      - /dev/null 
    depends_on:
      - sparkmaster

