#!/usr/bin/env bash
AIRFLOW_UI_PORT=5000
AIRFLOW_ADMIN_USR="admin"
AIRFLOW_ADMIN_PWD="admin"
AIRFLOW_API_BASE_URI="api/v1"
# https://stackoverflow.com/a/4774063
SCRIPT_DIR="$( cd -- "$(dirname "$0")" >/dev/null 2>&1 ; pwd -P )"

set -e
# load and export environment variables
# priority order:
#     .whirl.env in home directory
#     .whirl.env in whirl repository directory
#     .whirl.env in environment directory
#     .whirl.env in dag directory
#     already set PYTHON/AIRFLOW version env vars take precedence over everything else
function export_environment_vars() {
  CURRENT_ENV_PYTHON_VERSION=${PYTHON_VERSION}
  CURRENT_ENV_AIRFLOW_VERSION=${AIRFLOW_VERSION}
  set -a
  # shellcheck disable=SC2034
  DOCKER_CONTEXT_FOLDER=${SCRIPT_DIR}/docker
  # shellcheck disable=SC2034
  WHIRL_INITIATOR=$(whoami)
  # shellcheck disable=SC2034
  WHIRL_SETUP_FOLDER=/etc/airflow/whirl.setup.d

  # shellcheck source=${HOME}/.whirl.env
  # shellcheck disable=SC1091
  if [ -f ~/.whirl.env ]; then
    echo "Reading configuration from ~/.whirl.env"
    . ~/.whirl.env
  fi
  # shellcheck source=./.whirl.env
  if [ -f "${SCRIPT_DIR}/.whirl.env" ]; then
    echo "Reading configuration from ${SCRIPT_DIR}/.whirl.env"
    . "${SCRIPT_DIR}/.whirl.env"
  fi

  # determine whether to use the example set at the commandline or
  # in the current folder
  if [ -z "${WHIRL_EXAMPLE_ARG}" ]; then
    # shellcheck disable=SC2034
    DAG_FOLDER=$(pwd)
  else
    DAG_FOLDER=${SCRIPT_DIR}/examples/${WHIRL_EXAMPLE_ARG}
  fi
  # shellcheck disable=SC2034
  PROJECTNAME=$(basename "${DAG_FOLDER}")

  # determine whether to use the environment set at the commandline or
  # in the DAG FOLDER .whirl.env
  if [ -z "${WHIRL_ENVIRONMENT_ARG}" ]; then
    if [ -f "${DAG_FOLDER}/.whirl.env" ]; then
      if grep -qRE "^(\s+)?WHIRL_ENVIRONMENT=(.*)" "${DAG_FOLDER}/.whirl.env"; then
          echo "Found WHIRL_ENVIRONMENT in ${DAG_FOLDER}/.whirl.env";
          WHIRL_ENVIRONMENT=$(grep -oRE "^(\s+)?WHIRL_ENVIRONMENT=(.*)" "${DAG_FOLDER}/.whirl.env" | \
                              sed -e 's/.*=\(.*\)/\1/g')
          echo "environment is ${WHIRL_ENVIRONMENT}"
      fi
    fi
  else
    WHIRL_ENVIRONMENT=${WHIRL_ENVIRONMENT_ARG}
  fi

  # determine whether to use the environment dir set at the commandline or
  # in the DAG FOLDER .whirl.env
  echo ""
  if [ -z "${WHIRL_ENVIRONMENT_DIR_ARG}" ]; then
    if [ -f "${DAG_FOLDER}/.whirl.env" ]; then
      if grep -qRE "^(\s+)?WHIRL_ENVIRONMENT_DIR=(.*)" "${DAG_FOLDER}/.whirl.env"; then
          echo "Found WHIRL_ENVIRONMENT_DIR in ${DAG_FOLDER}/.whirl.env";

          grep -oRE "^(\s+)?WHIRL_ENVIRONMENT_DIR=(.*)" "${DAG_FOLDER}/.whirl.env";
          grep -oRE "^(\s+)?WHIRL_ENVIRONMENT_DIR=(.*)" "${DAG_FOLDER}/.whirl.env" | sed -e 's/.*=\(.*\)/\1/g';
          WHIRL_ENVIRONMENT_DIR=$(grep -oRE "^(\s+)?WHIRL_ENVIRONMENT_DIR=(.*)" "${DAG_FOLDER}/.whirl.env" | \
                              sed -e 's/.*=\(.*\)/\1/g')
          echo "environment directory is ${WHIRL_ENVIRONMENT_DIR}"
      fi
    fi
  else
    WHIRL_ENVIRONMENT_DIR=${WHIRL_ENVIRONMENT_DIR_ARG}
  fi

  if [[ -z "${WHIRL_ENVIRONMENT_DIR}" ]]; then
    WHIRL_ENVIRONMENT_DIR=${SCRIPT_DIR}/envs
  fi
  ENVIRONMENT_FOLDER=${WHIRL_ENVIRONMENT_DIR}/${WHIRL_ENVIRONMENT};

  if [[ -z "${WHIRL_ENVIRONMENT}" || ! -d ${ENVIRONMENT_FOLDER} ]]; then
    echo "No valid environment '${WHIRL_ENVIRONMENT}' specified"
    exit 2;
  fi

  if [ -f "${ENVIRONMENT_FOLDER}/.whirl.env" ]; then
    echo "Reading configuration from ${ENVIRONMENT_FOLDER}/.whirl.env"
    . "${ENVIRONMENT_FOLDER}/.whirl.env"
  fi

  if [ -f "${DAG_FOLDER}/.whirl.env" ]; then
    echo "Reading configuration from ${DAG_FOLDER}/.whirl.env"
    . "${DAG_FOLDER}/.whirl.env"
  fi

  # in case DAG_FOLDER specifies WHIRL_ENVIRONMENT, commandline needs to overrule if set
  if [[ -n "${WHIRL_ENVIRONMENT_ARG}" ]]; then
    WHIRL_ENVIRONMENT=${WHIRL_ENVIRONMENT_ARG}
  fi

  if [[ -n "${CURRENT_ENV_PYTHON_VERSION}" ]]; then
    echo "PYTHON_VERSION ${CURRENT_ENV_PYTHON_VERSION} was specified in env prior to execution. Overriding the one specified in .env files"
    PYTHON_VERSION=${CURRENT_ENV_PYTHON_VERSION}
  fi

  if [[ -n "${CURRENT_ENV_AIRFLOW_VERSION}" ]]; then
    echo "AIRFLOW_VERSION ${CURRENT_ENV_AIRFLOW_VERSION} was specified in env prior to execution. Overriding the one specified in .env files"
    AIRFLOW_VERSION=${CURRENT_ENV_AIRFLOW_VERSION}
  fi
  set +a
}

detect_potential_dag() {
  test "$(find "${DAG_FOLDER}" -type f -name '*.py' -maxdepth 1 -o -name '*.zip' | wc -l)" -gt 0
}

check_next_dagrun_scheduled_today() {
  local DAG_ID=$1
  echo "Checking next dagrun for ${DAG_ID}" >&2
  result=$(curl -s -u ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/dags/${DAG_ID}" | jq '.next_dagrun // (now | todate) | split("T") | .[0] | strptime("%Y-%m-%d") | strftime("%Y%m%d") | tonumber - (now | strftime("%Y%m%d") | tonumber)')
  echo "Result of checking next dagrun for ${DAG_ID} is ${result} (days from now)" >&2
  if [ "${result}" -le 0 ]; then
    true
  else
    false
  fi
}

trigger_dagrun() {
  local DAG_ID=$1
  echo "Manually triggering dagrun for ${DAG_ID}" >&2
  result=$(curl -s -X POST -H "Content-Type: application/json" -u ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/dags/${DAG_ID}/dagRuns" -d "{}" | jq)
  echo "Result of triggering dagrun for ${DAG_ID} is ${result}" >&2
  sleep 5;
}

test_dag_state() {
  local DAG_ID=$1
  local STATE=$2
  local SHOW_RESULT=${3:-false}
  echo "Checking dag state(${STATE}) for ${DAG_ID}" >&2
  result=$(curl -s -u ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/dags/${DAG_ID}/dagRuns" | jq ".dag_runs | map(select(.state == \"${STATE}\")) | length")
  echo "Result of checking dag state(${STATE}) for ${DAG_ID} is ${result}" >&2
  if [ "${SHOW_RESULT}" == true ]; then
    output=$(curl -s -u ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/dags/${DAG_ID}/dagRuns" | jq ".dag_runs" )
    echo "Dagruns: $output" >&2
    output=$(curl -s -u ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/dags/${DAG_ID}/dagRuns/~/taskInstances" | jq ".task_instances" )
    echo "Tasks: $output" >&2
  else
    echo "$result"
  fi
}

test_dag_errors() {
  local DAG_ID=$1
  local SHOW_RESULT=${2:-false}
  echo "Checking dag ${DAG_ID} for errors." >&2
  result=$(curl -s -u ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/importErrors" | jq ".total_entries")
  echo "Result of checking dag errors for ${DAG_ID} is ${result}" >&2
  if [ "${SHOW_RESULT}" == true ]; then
    output=$(curl -s -u ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/importErrors" | jq )
    echo "ImportErrors: $output" >&2
  else
    echo "$result"
  fi
}

debug_log_airflow() {
  echo "Available dags:"
  curl -X GET -H 'Content-Type: application/json' --user ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/dags" || true
  echo "Dag Import Errors:"
  curl -X GET -H 'Content-Type: application/json' --user ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/importErrors" || true
  echo "Docker containers running:"
  docker ps
  echo "Airflow container logs:"
  docker-compose -f "${ENVIRONMENT_FOLDER}/docker-compose.yml" logs airflow
}

run_compose_setup_scripts() {
  if [[ -d ${ENVIRONMENT_FOLDER}/compose.setup.d ]]; then
    echo "============================================"
    echo "== Setup docker-compose specifics =========="
    echo "============================================"
    for filename in "${ENVIRONMENT_FOLDER}"/compose.setup.d/*.sh; do
      echo "Executing compose prepare script: $filename"
      # shellcheck disable=SC1090
      . "$filename"
    done
  fi
}

check_dagrun_result() {
  local DAG_ID=${1}
  local RETRY=${2:-true}

  while [ "$(test_dag_state "${DAG_ID}" "running")" -gt 0 ]; do
    echo "Dag '${DAG_ID}' is (still) running..."
    sleep 5;
  done
  SUCCES_STATE="success"
  FAILURE_STATE="failed"
  if [ "${WHIRL_CI_EXPECT_FAILURE}" == true ]; then
    SUCCES_STATE="failed"
    FAILURE_STATE="success"
  fi
  if [ "$(test_dag_state "${DAG_ID}" "${SUCCES_STATE}")" -ge 1 ]; then
    echo "Dag '${DAG_ID}' run(s) successfully finished"
    stop
    return 0
  fi
  if [ "$(test_dag_state "${DAG_ID}" "${FAILURE_STATE}")" -ge 1 ]; then
    echo "Dag '${DAG_ID}' run(s) failed"
    # output dagrun result
    test_dag_state "${DAG_ID}" "${FAILURE_STATE}" true
    return 1
  else
    echo "Dag '${DAG_ID}' neither success nor failed!!!"
    if ${RETRY}; then
      echo "Maybe there was no dagrun for dag ${DAG_ID}. Retry after manual trigger"
      trigger_dagrun "${DAG_ID}"
      check_dagrun_result "${DAG_ID}" false
    else
      # output dagrun result
      test_dag_state "${DAG_ID}" "${FAILURE_STATE}" true
      debug_log_airflow
      return 7
    fi
  fi
}

function version_lt() {
  local VER1="${1}"
  local VER2="${2}"
  test "$(echo "${VER1} ${VER2}" | tr " " "\n" | sort -rV | head -n 1)" != "${VER1}";
}

start() {
    echo "Starting airflow local run for environment ${WHIRL_ENVIRONMENT}"

    if [ "${CI_MODE}" == true ]; then
      if version_lt "${AIRFLOW_VERSION}" "${MINIMAL_AIRFLOW_VERSION}"; then
        echo "${AIRFLOW_VERSION} less then minimal version ${MINIMAL_AIRFLOW_VERSION}";
        echo "Skipping running in CI MODE....";
        exit 0;
      fi
    fi

    # Possible cleanup before starting
    run_compose_setup_scripts

    docker build --build-arg PYTHON_VERSION="${PYTHON_VERSION}" --build-arg AIRFLOW_VERSION="${AIRFLOW_VERSION}" -t "docker-whirl-airflow:py-${PYTHON_VERSION}-local" "${SCRIPT_DIR}/docker/airflow-python"

    DAEMON=""
    if [ "${CI_MODE}" == true ]; then
      if [ -z "${DAG_ID}" ]; then
        DAG_ID=$(grep -Eo "dag_id=['\"](.*)['\"]" "${DAG_FOLDER}/dag.py" | sed -E "s/.*=['\"](.*)['\"]/\1/")
        if [ -z "${DAG_ID}" ]; then
          echo "Unable to determine dag id for CI_MODE and DAG_ID not provided"
          exit 4
        else
          echo "Found dag_id: ${DAG_ID} in dag file."
        fi
      fi
      DAEMON="-d"
      export AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth
    fi

    docker-compose -f "${ENVIRONMENT_FOLDER}/docker-compose.yml" up ${DAEMON} --build --force-recreate

    if [ "${CI_MODE}" == true ]; then
      wait_period=0
      while [[ "$(curl -s -o /dev/null -w %\{http_code\} http://localhost:${AIRFLOW_UI_PORT})" != "302" ]]; do
        echo "Waiting for Airflow UI to come up..."
        if [ $wait_period -ge 600 ]; then
          status_code="$(curl -s -o /dev/null -w %\{http_code\} http://localhost:${AIRFLOW_UI_PORT} || true)"
          echo "Been waiting for airflow to start for 10 minutes. Aborting now..."
          echo "Current HTTP status of http://localhost:${AIRFLOW_UI_PORT} is ${status_code}"
          debug_log_airflow
          stop
          exit 3
        else
          wait_period=$((wait_period+10))
          sleep 10;
        fi
      done
      sleep 10;
      if [ "${WHIRL_CI_EXPECT_IMPORTERRORS}" == true ]; then
        echo "Checking dag ${DAG_ID} for expected errors."
        result=$(curl -s -u ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/importErrors" | jq ".total_entries")
        echo "Result of checking dag ${DAG_ID} for expected errors is ${result}" >&2
        if [ "$(test_dag_errors "${DAG_ID}")" -gt 0 ]; then
          echo "Dag ${DAG_ID} has expected errors. Stopping..."
          stop
          exit 0
        else
          echo "Dag ${DAG_ID} did not have expected errors."
          test_dag_errors "${DAG_ID}" true
          stop
          exit 8
        fi
      else
        if [[ "$(curl -s -o /dev/null -w %\{http_code\} -X PATCH  -H 'Content-Type: application/json' --user ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/dags/${DAG_ID}?update_mask=is_paused" -d '{ "is_paused": false }')" != "200" ]]; then
          echo "Unable to unpause dag with id ${DAG_ID}."
          echo "Command: curl -X PATCH  -H 'Content-Type: application/json' --user ${AIRFLOW_ADMIN_USR}:****** \"http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/dags/${DAG_ID}?update_mask=is_paused\" -d '{ \"is_paused\": false }'"
          echo "Output:"
          curl -X PATCH  -H 'Content-Type: application/json' --user ${AIRFLOW_ADMIN_USR}:${AIRFLOW_ADMIN_PWD} "http://localhost:${AIRFLOW_UI_PORT}/${AIRFLOW_API_BASE_URI}/dags/${DAG_ID}?update_mask=is_paused" -d '{ "is_paused": false }'
          debug_log_airflow
          stop
          exit 5
        else
          echo "Unpaused dag with id ${DAG_ID}."
          sleep 5;
          if ! check_next_dagrun_scheduled_today "${DAG_ID}"; then
              echo "Need to trigger the dag ${DAG_ID} since first scheduled time is later than today and unpause is not enough to start running"
              trigger_dagrun "${DAG_ID}"
          fi

          check_dagrun_result "${DAG_ID}"
          exit ${?}
        fi
      fi
    fi

    echo "Closing down the environment"
    docker-compose -f "${ENVIRONMENT_FOLDER}/docker-compose.yml" down
}

stop() {
  echo "Stopping airflow-localrun containers..."
  docker-compose -f "${ENVIRONMENT_FOLDER}/docker-compose.yml" down --volumes --rmi local --remove-orphans
}

logs() {
  echo "Showing logs of airflow-localrun service ${WHIRL_SERVICE_NAME}"
  docker-compose -f "${ENVIRONMENT_FOLDER}/docker-compose.yml" logs -f "${WHIRL_SERVICE_NAME}"
}

usage() {
  echo "usage: ${BASH_SOURCE[0]} [-h|--help] [-e|--environment env] [start|stop|ci]"
  echo "  -h|--help                          display usage"
  echo "  -e|--environment environment       specify environment to use"
  echo "  -d|--directory environment_folder  specify the folder that contains the environments (defaults to SCRIPT_DIR)"
  echo "  -l|--logs servicename              tail the logs of the service"
  echo "  start|stop                         start or stop all"
  echo "  ci                                 runs in daemonized mode and awaits dag run completion"
  echo "  -i|--dag_id dag_id                 specify the dag_id to check (only in ci mode)"
  exit 21
}

function read_arguments() {
  while [[ $# -gt 0 ]]
  do
      key="${1}"
      case ${key} in
      -x|--example)
          WHIRL_EXAMPLE_ARG="${2}"
          shift # past argument
          shift # past value
          ;;
      -e|--environment)
          WHIRL_ENVIRONMENT_ARG="${2}"
          shift # past argument
          shift # past value
          ;;
      -d|--directory)
          WHIRL_ENVIRONMENT_DIR_ARG="${2}"
          shift # past argument
          shift # past value
          ;;
      -l|--logs)
          LOGS=true
          WHIRL_SERVICE_NAME="${2}"
          shift # past argument
          shift # past value
          ;;
      -i|--dag_id)
          DAG_ID="${2}"
          shift # past argument
          shift # past value
          ;;
      start)
          CI_MODE=false
          shift
          ;;
      ci)
          CI_MODE=true
          shift # past argument
          ;;
      stop)
          STOP=true
          shift # past argument
          ;;
      -h|--help)
          usage
          ;;
      *)  # unknown option
          echo "WARNING: Skipping unknown commandline argument: '${key}'"
          shift # past argument
          ;;
      esac
  done
}

function main() {
  read_arguments "$@"

  export_environment_vars
  if detect_potential_dag; then

    if [ -z "${LOGS}" ]; then
      if [ -z "${STOP}" ]; then
        start
      else
        stop
      fi
    else
      logs
    fi
  else
    echo "No .py or .zip files found in ${DAG_FOLDER} that may contain an Apache Airflow DAG"
    echo "did you correctly specify the example directory?"
  fi
}

main "$@"
